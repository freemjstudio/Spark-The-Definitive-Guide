# 개요
- 스파크에서 기본적으로 지원하는 데이터 소스들에 대하여 소개한다.
- 스파크의 핵심 데이터 소스로는 다음과 같다.
  - CSV, JSON, Parquet, ORC< JDBC/ODBC, Text
- 스파크 커뮤니티에서 만든 수많은 데이터 소스가 존재하는데 그 중 일부는 다음과 같다.
  - Cassandra, HBase, MongoDB, AWS Redshift, XML 등
- 기본적으로 데이터 소스를 이용해 데이터를 읽고 쓰는 방법을 터득한다.
- Third parth 데이터 소스와 스파크를 연동할 때 무엇을 고려할지 배운다.
- 실습 환경은 다음과 같다.
  - DataBricks Community Edition
# 1. 데이터소스 API
## 2.1. 읽기 (Read)
- 스파크에서 기본적으로 데이터를 읽을 때는 DataFrameReader 객체를 사용한다.
- SparkSession의 read 메서드를 호출하면 DataFrameReader 객체를 생성한다.
  ```scala
  spark.read
    .format("csv")
    .option("mode", "FAILFAST")
    .option("inferSchema", "true")
    .option("path", "path/to/file")
    .schema(someSchema)
    .load()
  ```
- DataFrameReader 객체를 생성하게 되면 다음과 같은 값을 지정해야 한다.
  - 포맷 (optional)
  - 스키마 (optional)
  - 읽기 모드
  - 옵션 (optional)
- 포맷은 기본적으로 Parquet 포맷을 사용한다.
- option 메서드를 통해 데이터를 읽는 방법에 대하여 파라미터를 키-값 쌍으로 설정할 수 있다.
- 데이터 소스를 읽을 때 스파크에 정의된 형식에 맞지 않는 데이터를 만났을 때 동작 방식을 지정하는 읽기 모드는 다음과 같다.
  - permissive (default) : 오류 레코드의 모든 필드를 null로 설정하고 모든 오류 레코드를 _corrupt_record라는 문자열 컬럼에 기록한다.
  - dropMalformed : 형식에 맞지 않는 레코드가 포함된 로우를 제거한다.
  - failFast : 형식에 맞지 않는 레코드를 만나면 즉시 종료한다.
- 예시
  #### sample.txt
  ```text
  id,name,doj,salary
  1,a,2020/08/12,1000
  2,b,2020/08/13,1000
  3,c,2020/08/13,1000
  4,d,2020/08/15,1000
  5,e,2020/08/16,1k
  ```
  ```scala
  spark.read
    .schema("id Integer, name String, dog Date, salary Integer, _corrupt_record String")
    .format("csv")
    .option("mode", "PERMISSIVE")
    .option("inferSchema", "true")
    .option("header", "true")
    .load("dbfs:/FileStore/data/sample.txt")
    .show()
  ```
  ```text
  +---+----+----+------+-------------------+
  | id|name| dog|salary|    _corrupt_record|
  +---+----+----+------+-------------------+
  |  1|   a|null|  1000|1,a,2020/08/12,1000|
  |  2|   b|null|  1000|2,b,2020/08/13,1000|
  |  3|   c|null|  1000|3,c,2020/08/13,1000|
  |  4|   d|null|  1000|4,d,2020/08/15,1000|
  |  5|   e|null|  null|  5,e,2020/08/16,1k|
  +---+----+----+------+-------------------+
  ```
## 2.2. 쓰기 (Write)
- 스파크에서 기본적으로 데이터를 읽을 때는 DataFrameWriter 객체를 사용한다.
- SparkSession의 write 메서드를 호출하면 DataFrameWriter 객체를 생성한다.
  ```scala
  spark.write
    .format("csv")
    .option("mode", "OVERWRITE")
    .option("dataFromat", "yyyy-MM-dd")
    .option("path", "path/to/file")
    .save()
  ```
- DataFrameWriter 객체를 생성하게 되면 다음과 같은 값을 지정해야 한다. 
  - 포맷 (optional)
  - 옵션 (optional)
  - 저장 모드 
- 포맷은 기본적으로 Parquet 포맷을 사용한다. 
- option 메서드를 통해 데이터를 쓰는 방법에 대하여 파라미터를 키-값 쌍으로 설정할 수 있다. 
- 데이터 소스를 쓸 때 동일한 파일이 발견되었을 때의 동작 방식을 지정하는 쓰기 모드는 다음과 같다.
  - append : 해당 경로에 이미 존재하는 파일 목록에 결과 파일을 추가한다.
  - overwrite : 이미 존재하는 모든 데이터를 완전히 덮어쓴다.
  - errorIfExists (default) : 해당 경로에 데이터나 파일이 존재하는 경우 오류를 발생시키면서 쓰기 작업이 실패한다.
  - ignore : 해당 경로에 데이터나 파일이 존재하는 경우 아무런 처리도 하지 않는다.
- 예시
  #### sample.txt
  ```text
  id,name,doj,salary
  1,a,2020/08/12,1000
  2,b,2020/08/13,1000
  3,c,2020/08/13,1000
  4,d,2020/08/15,1000
  5,e,2020/08/16,1k
  ```
  ```scala
  spark.read
    .schema("id Integer, name String, dog Date, salary Integer, _corrupt_record String")
    .format("csv")
    .option("mode", "PERMISSIVE")
    .option("inferSchema", "true")
    .option("header", "true")
    .load("dbfs:/FileStore/data/sample.txt")
    .show()
  ```
  ```text
  +---+----+----+------+-------------------+
  | id|name| dog|salary|    _corrupt_record|
  +---+----+----+------+-------------------+
  |  1|   a|null|  1000|1,a,2020/08/12,1000|
  |  2|   b|null|  1000|2,b,2020/08/13,1000|
  |  3|   c|null|  1000|3,c,2020/08/13,1000|
  |  4|   d|null|  1000|4,d,2020/08/15,1000|
  |  5|   e|null|  null|  5,e,2020/08/16,1k|
  +---+----+----+------+-------------------+
  ```
# 2. CSV
- CSV(commna-separated values)는 콤마(,)로 구분된 값을 의미한다.
- 각 줄이 단일 레코드가 되며 각 필드를 콤마로 구분하는 일반적인 텍스트 파일 포맷이다.
- 까다로운 파일 포맷으로 이는 어떤 내용이 들어있는지, 어떤 구조로 되어있는지 등 다양한 전제를 만들어낼 수 없기 때문이다. 
  - 파일 내용에 콤마(,)가 들어있다거나 비표준적인 방식으로 null이 기록되는 경우
- 이러한 문제를 해결하기 위해 다양한 옵션들을 소개한다.

  |read/write|Key|Value| Default                    | 설명                                                                         |
  |---|---|---|----------------------------|----------------------------------------------------------------------------|
  |all|sep|단일 문자| ,                          | 각 필드와 값을 구분하는데 사용되는 단일 문자                                                  |
  |all|header|true/false| false                      | 첫 번째 줄이 컬럼명인지 나타내는 불리언값                                                    |
  |read|escape|모든 문자열| \                          | 스파크에서 이스케이프 처리할 문자                                                         |
  |read|inferSchema|true/false| false                      | 스파크가 파일을 읽을 때 컬럼의 데이터 타입을 추론할 지 정의                                         |
  |read|ignoreLeadingWhiteSpace|true/false| false                      | 값을 읽을 때 값의 선행 공백을 무시할지 정의                                                  |
  |read|ignoreTrailingWhiteSpace|true/false| false                      | 값을 읽을 때 값의 후행 공백을 무시할지 정의                                                  |
  |all|nullValue|모든 문자열| ""                         | 파일에서 null 값을 나타내는 문자                                                       |
  |all|nanValue|모든 문자열| NaN                        | CSV 파일에서 NaN이 값없음을 나타내는 문자를 선언                                             |
  |all|positiveInf|모든 문자열 또는 문자| Inf                        | 양의 무한 값을 나타내는 문자(열)을 선언                                                    |
  |all|negativeInf|모든 문자열 또는 문자| -Inf                       | 음의 무한 값을 나타내는 문자(열)을 선언                                                    |
  |all|compression/codec|none, uncompressed, bzip2, defalte, gzip, lz4, snappy| none                       | 스파크가 파일을 읽고 쓸 때 사용하는 압축 코덱을 정의                                             |
  |all|dateFormat|자바의 SimpleDataFormat을 따르는 모든 문자열 또는 문자| yyyy-MM-dd                 | 날짜 데이터 타입인 모든 필드에서 사용할 날짜 형식                                               |
  |all|timestampFormat|자바의 SimpleDataFormat을 따르는 모든 문자열 또는 문자| yyyy-MM-dd'T'HH:mm:ss.SSSZZ | 타임스탬프 데이터 타입인 모든 필드에서 사용할 날짜 형식                                            |
  |read|maxColumns|모든 정수| 20480                      | 파일을 구성하는 최대 컬럼 수를 선언                                                       |
  |read|maxCharsPerColumn|모든 정수| 1000000                    | 컬럼의 문자 최대 길이를 선언                                                           |
  |read|escapeQuotes|true/false| true                       | 스파크가 파일의 라인에 포함된 인용부호를 이스케이프 할 지 선언                                        |
  |read|maxMalformedLogPerPartition|모든 정수| 10                         | 스파크가 각 파티션 별로 비정상적인 레코드를 발견했을 때 기록할 최대 수. 해당 숫자를 초과하는 비 정상적인 레코드는 모두 무시된다. |
  |write|quoteAll|true/false| false                      | 인용부호 문자가 있는 값을 이스케이프 처리하지 않고 전체 값을 인용 부호로 묶을지 여부                           |
  |read|multiLine|true/false| false                      | 하나의 논리적 레코드가 여러 줄로 이어진 CSV 파일 읽기를 허용할지 여부                                  |

- 읽기 예시
  ```scala
  import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}

  val myManualSchema = new StructType(Array(
  new StructField("DEST_COUNTRY_NAME", StringType, true),
  new StructField("ORIGIN_COUNTRY_NAME", StringType, true),
  new StructField("count", LongType, true)))
  
  val csv_df = spark.read.format("csv")
  .option("header", "true")
  .option("mode", "FAILFAST")
  .schema(myManualSchema)
  .load("dbfs:/FileStore/data/2010_summary.csv")
  
  csv_df.show()
  ```
- 쓰기 예시
  ```scala
  csv_df.write.format("csv").mode("overwrite").option("sep", "\t").save("/tmp/2010_summary.csv")
  ```
- 스파크는 지연 연산 특성이 있으므로 데이터 프레임을 정의하는 시점이 아닌 잡 실행 시점에만 오류가 발생한다.
# 3. JSON
- JSON(JavaScript Object Notion)은 기본적으로 줄로 구분된(line-delimidated) JSON을 기본적으로 사용한다.
- 또한, multiLine 옵션을 사용하여 줄로 구분된 방식과 여러 줄로 구분된 방식을 선택적으로 사용할 수 있다.
- 줄로 구분된 JSON의 포맷이 안정적인 포맷이므로 이 방식을 사용하는 것을 추천한다.
- JSON 객체를 다룰 때 사용하는 옵션은 다음과 같다.

  |read/write|Key|Value| Default                    | 설명                                                                         |
  |---|---|---|----------------------------|----------------------------------------------------------------------------|
  |all|compression/codec|none, uncompressed, bzip2, defalte, gzip, lz4, snappy|none|스파크가 파일을 읽고 쓸 때 사용하는 압축 코덱을 정의|
  |all|dateFormat|자바의 SimpleDataFormat을 따르는 모든 문자열 또는 문자|yyyy-MM-dd|날짜 데이터 타입인 모든 필드에서 사용할 날짜 형식|
  |all|timestampFormat|자바의 SimpleDataFormat을 따르는 모든 문자열 또는 문자|yyyy-MM-dd'T'HH:mm:ss.SSSZZ|타임스탬프 데이터 타입인 모든 필드에서 사용할 날짜 형식|
  |read|primitiveAsString|ture/false|false|모든 primitive 값을 문자열로 추정할지 정의|
  |read|allowComments|true/false|false|JSON 레코드에서 자바나 C++ 스타일로 된 코멘트를 무시할지 정의|
  |read|alloUnquotedFieldNames|true/false|false|인용부호로 감싸여 있지 않은 JSON 필드명을 허용할지 정의|
  |read|allowSingleQuotes|true/false|true|인용부호로 큰따옴표(") 대신 작은따옴표(')를 허용할지 정의|
  |read|allowNumericLeadingZeros|true/false|false|숫자 앞에 0을 허용할지 정의|
  |read|alloBackslashEscapingAnyCharacter|true/false|false|백슬리시 인용부호 메커니즘을 사용한 인용부호를 허용할지 정의|
  |read|ColumnNameOfCorruptRecord|모든 문자열|spark.sql.columnNameOfCorruptRecord 속성의 설정값|permissive 모드에서 생성된 비정상 문자열을 가진 새로운 필드명을 변경할 수 있다. 해당 값을 설정하면 spark.sql.columnNameOfCorruptRecord 설정값 대신 적용|
  |read|multiLine|true/false|false|줄로 구분되지 않은 JSON 파일의 읽기를 허용할지 정의|

- 읽기 예시
  ```scala
  spark.read.format("json")
    .option("mode", "FAILFAST")
    .schema(myManualSchema)
    .load("dbfs:/FileStore/data/flight-data/json/2010_summary.json")
    .show(5) 
  ```
  ```text
  +-----------------+-------------------+-----+
  |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
  +-----------------+-------------------+-----+
  |    United States|            Romania|    1|
  |    United States|            Ireland|  264|
  |    United States|              India|   69|
  |            Egypt|      United States|   24|
  |Equatorial Guinea|      United States|    1|
  +-----------------+-------------------+-----+
  ```
- 쓰기 예시
  ```scala
  csv_df.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")
  ```
  ```shell
  %fs
  ls /tmp/my-json-file.json/
  ```
  ```text
  path,name,size,modificationTime
  dbfs:/tmp/my-json-file.json/_SUCCESS,_SUCCESS,0,1690830377000
  dbfs:/tmp/my-json-file.json/_committed_5178524152520899269,_committed_5178524152520899269,112,1690830377000
  dbfs:/tmp/my-json-file.json/_started_5178524152520899269,_started_5178524152520899269,0,1690830376000
  dbfs:/tmp/my-json-file.json/part-00000-tid-5178524152520899269-2cb0b929-046b-45a8-9157-077740e18ab0-4-1-c000.json,part-00000-tid-5178524152520899269-2cb0b929-046b-45a8-9157-077740e18ab0-4-1-c000.json,21353,1690830377000
  ```
  ```shell
  %fs
  head /tmp/my-json-file.json/part-00000-tid-5178524152520899269-2cb0b929-046b-45a8-9157-077740e18ab0-4-1-c000.json
  ```
  ```text
  {"DEST_COUNTRY_NAME":"United States","ORIGIN_COUNTRY_NAME":"Romania","count":1}
  {"DEST_COUNTRY_NAME":"United States","ORIGIN_COUNTRY_NAME":"Ireland","count":264}
  {"DEST_COUNTRY_NAME":"United States","ORIGIN_COUNTRY_NAME":"India","count":69}
  {"DEST_COUNTRY_NAME":"Egypt","ORIGIN_COUNTRY_NAME":"United States","count":24}
  ... (skip)
  ```
# 4. Parquet
- Parquet는 다양한 스토리지 최적화 기술을 제공하는 오픈소스로 만들어진 컬럼 기반의 데이터 저장 방식이다. 
- 저장소 공간을 절약할 수 있고 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있으며 컬럼 기반의 압축 기능을 제공한다. 
- 특히, 아파치 스파크와 호환이 잘되기 때문에 스파크의 기본 파일 포맷으로 사용된다. 
- Parquet는 JSON, CSV보다 훨씬 효율적으로 동작하므로 장기 저장용 데이터는 Parquet 포맷으로 저장하는 것을 추천한다. 
- 또한, 복합 데이터 타입(struct, array, map)을 지원한다. 
- Parquet 데이터를 다룰 때 사용하는 옵션은 다음과 같다.

  |read/write|Key|Value| Default                    | 설명                                                                         |
  |---|---|---|----------------------------|----------------------------------------------------------------------------|
  |all|compression/codec|none, uncompressed, bzip2, defalte, gzip, lz4, snappy|none|스파크가 파일을 읽고 쓸 때 사용하는 압축 코덱을 정의|
  |read|mergeSchema|true/false|spark.sql.parquet.mergeSchema 속성의 설정값|동일한 테이블이나 폴더에 신규 추가된 Parquet 파일에 컬럼을 점진적으로 추가할 수 있다. 이러한 기능을 활성화하거나 비활성화하기 위해 해당 옵션을 사용한다.|

  - Parquet 경우에는 데이터를 저장할 때 자체 스키마를 사용해 데이터를 저장하기 때문에 옵션이 거의 없다. 
  - DataFrame을 표현하기 위해 정확한 스키마가 필요한 경우에만 스키마를 설정한다. (schema-on-read 방식을 지원하기 때문에 사실 거의 필요는 없음)
  - Parquet 파일은 스키마가 파일 자체에 내장되어 있으므로 스키마 추정이 필요 없다.

- 읽기 예시
  ```scala
  spark.read.format("parquet")
    .load("dbfs:/FileStore/data/flight-data/parquet/2010-summary.parquet")
    .show(5)
  ```
  ```text
  +-----------------+-------------------+-----+
  |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
  +-----------------+-------------------+-----+
  |    United States|            Romania|    1|
  |    United States|            Ireland|  264|
  |    United States|              India|   69|
  |            Egypt|      United States|   24|
  |Equatorial Guinea|      United States|    1|
  +-----------------+-------------------+-----+
  only showing top 5 rows
    ```
- 쓰기 예시
  ```scala
  csv_df.write.format("parquet").mode("overwrite").save("/tmp/my-parquet-file.parquet")
  ```
  ```shell
  %fs
  ls FileStore/data/flight-data/parquet/2010-summary.parquet/
  ```
  ```text
  path,name,size,modificationTime
  dbfs:/tmp/my-parquet-file.parquet/_SUCCESS,_SUCCESS,0,1690831196000
  dbfs:/tmp/my-parquet-file.parquet/_committed_7876934312199867966,_committed_7876934312199867966,122,1690831196000
  dbfs:/tmp/my-parquet-file.parquet/_started_7876934312199867966,_started_7876934312199867966,0,1690831193000
  dbfs:/tmp/my-parquet-file.parquet/part-00000-tid-7876934312199867966-8f4c074c-aacf-4f26-906f-1e24c267c467-5-1-c000.snappy.parquet,part-00000-tid-7876934312199867966-8f4c074c-aacf-4f26-906f-1e24c267c467-5-1-c000.snappy.parquet,5488,1690831196000
  ```
# 5. ORC
- ORC는 하둡 워크로드를 위해 설계된 자기 기술적(self-describing)이며 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷이다. 
- 대규모 스트리밍 읽기에 최적화되어 있을 뿐만 아니라 필요한 로우를 신속하게 찾아낼 수 있는 기능이 통합되어 있다. 
- 스파크에서는 ORC 파일 포맷을 효율적으로 사용할 수 있으므로 별도의 옵션 지정 없이 데이터를 읽을 수 있다.
- Parquet 파일은 스파크에 최적화 된 반면 ORC는 Hive에 최적화되어 있다.

- 읽기 예시
  ```scala
  spark.read.format("orc")
    .load("dbfs:/FileStore/data/flight-data/orc/2010-summary.orc")
    .show(5)
  ```
  ```text
  +-----------------+-------------------+-----+
  |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
  +-----------------+-------------------+-----+
  |    United States|            Romania|    1|
  |    United States|            Ireland|  264|
  |    United States|              India|   69|
  |            Egypt|      United States|   24|
  |Equatorial Guinea|      United States|    1|
  +-----------------+-------------------+-----+
  ```
- 쓰기 예시
  ```scala
  csv_df.write.format("orc").mode("overwrite").save("/tmp/my-orc-file.orc")
  ```
  ```shell
  %fs 
  ls /tmp/my-orc-file.orc/
  ```
  ```text
  path,name,size,modificationTime
  dbfs:/tmp/my-orc-file.orc/_SUCCESS,_SUCCESS,0,1690832777000
  dbfs:/tmp/my-orc-file.orc/_committed_2739091522261783320,_committed_2739091522261783320,118,1690832777000
  dbfs:/tmp/my-orc-file.orc/_started_2739091522261783320,_started_2739091522261783320,0,1690832776000
  dbfs:/tmp/my-orc-file.orc/part-00000-tid-2739091522261783320-500ff9ea-c56d-4303-af08-696279992748-9-1-c000.snappy.orc,part-00000-tid-2739091522261783320-500ff9ea-c56d-4303-af08-696279992748-9-1-c000.snappy.orc,3929,1690832776000
  ```

# 6. SQL 데이터베이스
- 사용자는 SQL를 지원하는 다양한 시스템에 SQL 데이터 소스를 연결할 수 있다.
- 예를 들어 MySQL, PostgreSQL, Oracle 데이터베이스에 접속할 수 있다.
- 데이터베이스는 인증 정보나 접속과 관련된 옵션이 추가적으로 필요하고 실제로 데이터베이스 시스템에 접속이 가능한지 네트워크 상태를 확인해야 한다.
- 해당 단원에서는 SQLite 실행을 위한 참고용 샘플을 대상으로 실습을 진행한다.
- 데이터베이스의 데이터를 읽고 쓰기 위해서는 스파크 classpath에 데이터베이스의 JDBC 드라이버를 추가하고 적절한 JDBC Jar 파일을 제공해야 한다.
  - 예시 : PostgreSQL
  ```shell
  %sh
  ./bin/spark-shell \
  --driver-class-path postgresql-9.4.1207.jar \
  --jars postgresql-9.4.1207.jar
  ```
- JDBC 데이터 소스를 다룰 때 옵션은 다음과 같다.

  |속성명| 의미                                                                                                                                                                                                                                                                                     |
  |---|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  |url| 접속을 위한 JDBC URL. 소스 시스템에 특화된 설정은 URL에 지정할 수 있다. <br> ex) jdbc:postgresql://localhost/test?user=fred&password=secret|
  |dbtable| 읽을 JDBC 테이블을 설정한다. SQL 쿼리의 FROM 절에 유효한 모든 것을 사용할 수 있다. <br> 예를 들어 전체 테이블 대신 괄호 안에 서브쿼리를 사용할 수도 있다.|
  |driver| 지정한 URL에 접속할 때 사용할 JDBC 드라이버 클래스 명을 지정한다.|
  |partitionColumn,lowerBound, upperBound| 세 가지 옵션은 항상 같이 지정해야 하며, numPartitions 또한 반드시 지정해야 한다. <br> 이러한 속성은 다수의 워커에서 병렬로 테이블을 나눠 읽는 방법을 정의한다. partitionColumn은 반드시 해당 테이블의 수치형 컬럼이어야 한다. <br> lowerBound와 upperBound는 테이블의 로우를 필터링하는데 사용되는 것이 아니라 각 파티션의 범위를 결정하는데 사용된다. <br> 따라서 테이블의 모든 로우는 분할되어 반환된다. <br> 해당 옵션은 읽기에만 적용된다. |
  |numPartitions| 테이블의 데이터를 병렬로 읽거나 쓰기 작업에 사용할 수 있는 최대 파티션 수를 결정한다. <br> 해당 속성은 최대 동시 JDBC 연결 수를 결정한다. <br> 쓰기에 사용되는 파티션 수가 이 값을 초과하는 경우 쓰기 연산 전에 coalesce(numPartitions)를 실행하여 파티션 수를 해당 값에 맞게 줄이게 된다.|
  |fetchSize| 한 번에 얼마나 많은 로우를 가져올지 결정하는 JDBC의 fetch size를 설정한다.<br> 해당 옵션은 기본적으로 fetch size가 작게 설정된 JDBC 드라이버의 성능을 올리는데 도움이 된다. (Oracle default:10) <br> 해당 옵션은 읽기에만 적용된다.|
  |batchSize| 한 번에 얼마나 많은 로우를 저장할지 결정하는 JDBC의 batch size를 설정한다. <br> 해당 옵션은 JDBC 드라이버의 성능을 향상시킬 수 있다. <br> 해당 옵션은 쓰기에만 적용되며 기본값은 1000이다.|
  |isolationLevel| 현재 연결에 적용되는 트랜잭션 격리 수준을 정의한다. <br> 해당 옵션은 JDBC Connection 객체에서 정의하는 표준 트랜잭션 격리 수준에 해당하는 None, READ_COMMITTED, READ_UNCOMMITTED, REPETABLE_READ, SERIALIZABLE 중에 하나가 될 수 있다. (자세한 내용은 java.sql.Connection 문서를 참고)                                                                       |
  |truncate| JDBC writer 관련 옵션이다. SaveMode.Overwrite가 활성화 되면 스파크는 기존 테이블을 삭제하거나 재생성하는 대신 데이터베이스의 truncate 명령을 실행한다. <br> 이런 동작 방식이 더 효율적일 수 있으며 인덱스 같은 테이블 메타데이터가 제거되는 현상을 방지할 수 있다. <br> 하지만 신규 데이터가 현재 스키마와 다른 경우와 같이 일부 경우에는 정상적으로 동작하지 않을 수 있다. <br> 해당 옵션의 기본값은 false이며 쓰기에만 적용된다.             |
  |createTableOptions| JDBC write 관련 옵션이다. 해당 옵션을 지정하면 테이블 생성 시 특정 테이블의 데이터베이스와 파티션 옵션을 설정할 수 있다. <br> ex) CREATE TABLE t (name string) ENGINE=InnoDB <br> 해당 옵션은 쓰기에만 적용된다.|
  |createTableColumnTypes| 테이블을 생성할 때 기본값 대신 데이터베이스 컬럼 데이터 타입을 정의한다. <br> 데이터 타입 정보는 반드시 CREATE TABLE 구문에서 사용하는 컬럼 정의 구문과 동일한 형식으로 지정해야 한다. <br> 지정된 타입은 유효한 스파크 SQL 타입이어야 한다. <br> 해당 옵션은 쓰기에만 적용된다.|

- 읽기 예시
  ```scala
  val driver =  "org.sqlite.JDBC"
  val path = "tmp/my_sqlite.db"
  val url = s"jdbc:sqlite:/${path}"
  val tablename = "flight_info"
  ```
  ```scala
  import java.sql.DriverManager
  val connection = DriverManager.getConnection(url)
  connection.isClosed()
  connection.close()
  ```
  ```scala
  val dbDataFrame = spark.read.format("jdbc")
    .option("url", url)
    .option("dbtable", tablename)
    .option("driver",  driver)
    .load()
  
  dbDataFrame.show(5)
  ```
  ```text
  +-----------------+-------------------+-----+
  |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
  +-----------------+-------------------+-----+
  |    United States|            Romania|    1|
  |    United States|            Ireland|  264|
  |    United States|              India|   69|
  |            Egypt|      United States|   24|
  |Equatorial Guinea|      United States|    1|
  +-----------------+-------------------+-----+
  ```
  - 스파크에는 데이터베이스 테이블에서 스키마 정보를 읽어 테이블에 존재하는 컬럼의 데이터 타입을 스파크의 데이터 타입으로 변환한다.
  - 따라서 생성된 데이터 프레임은 스파크의 데이터 프레임 처럼 자유롭게 사용할 수 있다.
- 쓰기 예시
  ```scala
  val newPath = "jdbc:sqlite://tmp/my-sqlite.db"
  csv_df.write.mode("overwrite").jdbc(newPath, tablename, props)
  ```
  ```scala
  spark.read.jdbc(newPath, tablename, props).count() // 255
  ```
  ```text
  res131: Long = 255
  ```
## 7.1. 쿼리 푸시 다운
- 스파크는 DataFrame을 만들기 전에 데이터베이스 자체에서 데이터를 필터링하도록 만들 수 있다.
- 예를 들어, 아래 예제에서 사용한 쿼리의 실행 계획을 한 번 보면 테이블의 여러 컬럼 중 관련이 있는 컬럼만 선택한다는 것을 알 수 있다.
  ```scala
  dbDataFrame.select("DEST_COUNTRY_NAME").distinct().explain
  ```
  ```text
  == Physical Plan ==
  AdaptiveSparkPlan isFinalPlan=false
  +- HashAggregate(keys=[DEST_COUNTRY_NAME#733], functions=[])
    +- Exchange hashpartitioning(DEST_COUNTRY_NAME#733, 200), ENSURE_REQUIREMENTS, [plan_id=404]
        +- HashAggregate(keys=[DEST_COUNTRY_NAME#733], functions=[])
            +- Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#733] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
  ```
- 스파크는 특정 유형의 쿼리를 더 나은 방식으로 처리할 수 있다.
- Dataframe에 filter를 명시하면 스파크는 해당 필터에 대한 처리를 데이터베이스로 위임(push down) 한다.
  ```scala
  dbDataFrame.filter("DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')").explain
  ```
  ```text
  == Physical Plan ==
  *(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#733,ORIGIN_COUNTRY_NAME#734,count#735]
  PushedFilters: [*In(DEST_COUNTRY_NAME, [Anguilla,Sweden])], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:decimal(20,0)>
  ```
- 스파크의 모든 함수는 사용하는 SQL 데이터베이스에 맞게 변환하지 못하는 경우도 있다.
- 이러한 경우에는 전체 SQL 쿼리를 데이터베이스에 전달하여 DataFrame으로 전달받을 수 있다.
  ```scala
  val pushdownQuery = """(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info"""

  val dbDataFrame = spark.read.format("jdbc")
  .option("url", url).option("dbtable", pushdownQuery).option("driver",  driver)
  .load()

  dbDataFrame.show(5)
  ```
  ```text
  +-----------------+
  |DEST_COUNTRY_NAME|
  +-----------------+
  |    United States|
  |            Egypt|
  |Equatorial Guinea|
  |       Costa Rica|
  |          Senegal|
  +-----------------+

  ```
  ```scala
  dbDataFrame.explain
  ```
  ```text
  == Physical Plan ==
  *(1) Scan JDBCRelation((SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info)
  AS flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#779] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
  ```
## 7.2. 데이터베이스 병렬로 읽기


# 7. 텍스트 파일

# 8. 고급 I/O 개념

# 정리